llm:
  # If set, disables LLM and returns provided response.
  # If it's an empty string, the server will return the user's prompt.
  # Remove this config entry to use LLM.
  mocked_response: 'Hello, this is a sample response'
  # LLM ollama model
  # https://ollama.com/library/gemma
  model: 'gemma:2b-instruct'
  # The temperature of the model. Increasing the temperature will make the model answer more creatively.
  temperature: 0.7
  # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative.
  top_k: 40
  # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.
  top_p: 0.9
  api: 'http://localhost:11434'
  # How many question-response pairs to remember?
  # Too high, and the LLM will not deviate from topics.
  # Too low and the conversation will be 'random'.
  context_length: 10
  # Message added before each user question
  system_message: >-
    Answer the question using your general knowledge.
    If you don't know the answer, just say that you don't know.
    Use three sentences maximum and keep the answer concise.

tts:
  use_gpu: False
  model_name: 'tts_models/en/ljspeech/tacotron2-DDC'
  # Required for multispeaker models (like xtts_v2)
  # Call `make xtts-list-speakers` to get speakers list.
  # Or call `make xtts-create-speaker-samples` to generate samples
  # so you can manually pick pick best one.
  speaker: 'Rosemary Okafor'

  # Required in multilingual models (like xtts_v2)
  language: 'en'

  # Optional file that contains voice we will be cloning
  # sample_of_cloned_voice_wav: 'voice_to_clone.wav'

  # Allow to use DeepSpeed library for faster TTS inference
  # https://github.com/microsoft/DeepSpeed
  deepspeed_enabled: True

  # Split text into smaller chunks to stream them faser. Lower chunk size
  # reduces time to first sound, but might produce glitches between
  # chunks.
  # Recommended ON, but make sure that GPU can infer in real time (deepspeed helps).
  streaming_enabled: False
  # GPT generates sound in chunks. Collect $chunk_size of such chunks
  # and send them to the client.
  # - small chunk: faster first sound, might introduce 'glitches'
  #       between chunks. More inconsistencies between speech styles.
  #       E.g. 20 is quite tiny
  # - larger chunk: slower first sound, but more 'consistent' throught.
  streaming_chunk_size: 20
  # crossfade between 2 consequtive chunks to prevent 'glitches'
  # when next chunk has different speech style.
  streaming_overlap_wav_len: 1024

server:
  host: 'localhost'
  port: 8080
