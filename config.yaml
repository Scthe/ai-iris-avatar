# verbose: False

llm:
  # if disabled, it will just read out the question
  enabled: True
  # LLM ollama model
  model: 'gemma:2b'
  # The temperature of the model. Increasing the temperature will make the model answer more creatively.
  temperature: 0.7
  # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative.
  top_k: 40
  # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.
  top_p: 0.9
  api: 'http://localhost:11434'

tts:
  use_gpu: False
  model_name: 'tts_models/en/ljspeech/tacotron2-DDC'
  # vocoder_name: 'vocoder_models/en/ljspeech/hifigan_v2'
  # TODO?
  # voice_conversion_model: "voice_conversion_models/multilingual/vctk/freevc24"

  # Split llm answer by $chunk_size tokens. Each such chunk
  # is TTS'ed separately. Lower value means we start delivering voice faster,
  # but risk many unnatural pauses between chunks. Higher value
  # is more fluid, more resistent to variable processing time,
  # but has startup delay.
  #
  # If the value is 0, the whole answer will be TTSed at once (single chunk).
  # TTS will internally split this on sentences. Best for quality, but SLOW.
  #
  # TL;DR: Max LLM tokens before a flush to TTS
  chunk_size: 0

server:
  host: 'localhost'
  port: 8080
